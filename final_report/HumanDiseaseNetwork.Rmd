---
author: "Omar Ghetti 793181"
title: "Human Disease Network"
description: "Data Analitycs Final Project"
output:
  bookdown::pdf_book:
    includes:
      before_body: frontespizio.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This project aims to make a complete rundown over the Human Disease Network, exploring the data and evaluating clustering algorithms to find correlations between different nodes, that in this precise case are representing the different diseases.
Two diseases are connected to each other only if they share at least one gene in which mutations are associated with both diseases.Different State-of-the-art analysis over the network proved that some of those genes tend to group in well distinct clusters, while a big part of them tend to isolate themself at the borders of the network, or to avoid to group in general. The aim of the project is to validate those results and to test some community-detection algorithms over the network, to see if the results are comparable to the state-of-the-art analisys.
The network presented by the researchers presents 22 defined diseases classes, defining an initial groundtruth useful for the final comparison with the algorithms results.

# Data Exploration

## Network Exploration

First of all, a panoramic view on the data presented in the dataset is useful to start looking at the network, so a console printing of the data is presented, followed by a first network representation made with ggraph library

```{r include=FALSE}
## Libraries and Graph Loading
source("../Shared_Functions.R")
libraries <- c("ggraph","igraph","dplyr","readr", "DiagrammeR", "tidyverse", "Cairo", 
               "networkD3","CINNA","scales","pander")
import_libraries(libraries)
edges <- read.csv("../Dataset/diseasome [Edges].csv", head=TRUE)
nodes <- read.csv("../Dataset/diseasome [Nodes].csv", head=TRUE)

## Cleaning
nodes <- nodes %>% select(-timeset)
edges <- edges %>% select(-timeset,-label)

network_graph <- graph.data.frame(edges,directed = TRUE, vertices = nodes)
```

```{r echo=FALSE}
print(network_graph, e=TRUE, v=TRUE)

```

```{r graph,echo=FALSE,cache=TRUE}
ggraph(network_graph, layout="graphopt") +
  geom_edge_fan(colour="grey15",show.legend = TRUE) +
  geom_node_point(fill= ifelse(nodes$X0 == "gene", "#00FF00", "#0D98BA"), 
                  shape=23, col="grey15", show.legend = TRUE) +
  scale_size_continuous(range=c(1, 10)) +
  theme_graph(base_size = 11, base_family = "serif") +
  ggtitle("Human Disease Network")

```

In this graph, genes are highlighted in Green, while diseases are highlited in Blue. It's possible to see that the network is quite wide, consisting of more than 3000 nodes and more than 1000 vertices.
the gene class is also more represented than the disease one, presenting two times the number of gene. It's also important to note that we are working with a directed graph.

```{r echo=FALSE}
print(paste("Vertices Number:", vcount(network_graph)))
print(paste("Genes Number:", sum(V(network_graph)$X1 == "gene")))
print(paste("Diseases Number:", sum(V(network_graph)$X1!="gene")))
print(paste("Edges Number:", ecount(network_graph)))

```


# Centrality Analisys

## Degree Centrality

giving the fact that the graph is a directed graph, analisys over degree will take count of Outdegree and Indegree for a certain node, cause in the case of a directed graph both are present. Nodes with high degree will be considered as Hubs for the network, and the idea is that they will be central in clusters. values for degree centrality are normalized.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Degree Centrality
network_degree = centr_degree(network_graph,mode="all",normalized = TRUE)

histogram_plot(network_degree$res,seq(0,180,by=20),"n_degree","freq","Degree Centrality")
```


analyzing the results, it's clear that a lot of nodes have a centrality value close to 1, and, given the number, it's fair to say that genes are the main cause of this behaviour cause the majority of them have 1 as degree.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Directed Graph: In and Out Degree
odegree = centr_degree(network_graph, mode="out",normalized=TRUE)
histogram_plot(odegree$res, seq(0,20,by=20),"outdegree","freq","OutDegree")
idegree = centr_degree(network_graph, mode="in",normalized=TRUE)
histogram_plot(idegree$res, seq(0,20,by=20),"indegree","freq","InDegree")

```

looking specifically at indegree numbers and outdegree numbers, it's possible to see that no one of the genes present an outgoing edge, and they just have one entering edge. it's now presented a rank of the top and bottom 10 nodes for degree.

```{r echo=FALSE}
print("TOP 10 NODES FOR DEGREE")
V(network_graph)$label[order(network_degree$res, decreasing = TRUE)][1:10]
print("BOTTOM 10 NODES FOR DEGREE")
V(network_graph)$label[order(network_degree$res, decreasing = FALSE)][1:10]
```

as expected from the histograms, all the top 10 nodes are diseases, while all the bottom 10 are genes.

## Betweenness
Betweenness is a measure that represents how central is a node based on how many shortest paths on the totality of them present the selected node. this is a more robust metric to evaluate centrality of a node because is not only based on the node itsefl. The measure is calculated in his normalized variant.

```{r echo=FALSE, message=FALSE, warning=FALSE}
b_cetr = betweenness(network_graph,v=V(network_graph),directed=TRUE,normalized=TRUE)
histogram_plot(b_cetr, c(0,1),"betweenness","freq","Betweenness Centrality") 

```

as for the degree measure, a lot of nodes sits on a low value of betweenness. Just 5 of the nodes have a quite higher value of Betweenness, and it's expected that they will be hubs.
Those are the top nodes for Betweenness:

```{r echo=FALSE}
V(network_graph)$label[order(b_cetr, decreasing = TRUE)][1:10]

```

## Closeness

Closeness is a measure that highlights the ability of a node to spread informations effectively and quickly. Is based on the average farness of a node from the other ones. It's expected that nodes with high closeness have short distances from other nodes. This measure is pretty unrelevant when the network is undirected, but we are working over a dircted network so it's worth having this measure.

```{r echo=FALSE, message=FALSE, warning=FALSE}
c_centr <- closeness(network_graph, vids=V(network_graph),mode="all",normalized=TRUE)
histogram_plot(c_centr,c(0,1), "Closeness","Freq","Closeness")

```

The values of normalized closeness centrality are always between 0 and 1. In this case, values are balanced in the middle of the scale, with just 10 nodes scoring 0.

## Pagerank & Eigenvector Centrality

Pagerank and Eigenvector are two similar measures that are useful to understand how nodes influence other nodes in the network. Nodes with high Eigenvector scores are more likely to have lot of influence on other nodes in the network. The procedure is close to the degree centrality one, but it goes one step further, checking also connections of the nodes connected to the node in exam. A node can have an high degree but low Eigenvector, because it's connections could be with other low scoring nodes. Pagerank, invented by Google, differs from eigenvector because scores nodes on the incoming links, and so it's pretty unrelevant in undirected networks.

```{r echo=FALSE, message=FALSE, warning=FALSE}
eigenv <- eigen_centrality(network_graph,directed=TRUE)
histogram_plot(eigenv$vector, c(0,1),"eigen_N","Freq","Eigenvector")
prank <- page_rank(network_graph,vids=V(network_graph),directed = TRUE)
histogram_plot(prank$vector,c(0,1),"Pagerank","Freq","Pagerank")


```


as expected, a lot of nodes are scoring really low on both the measures, and there are just 4-5 nodes that are reaching high values. It's expected that those nodes will be very important inside the network. Graph is now plotted with node dimensions regulated by those two measures:

```{r graph_prank, echo=FALSE, warning=FALSE}
plot_graph(network_graph, "dh", prank$vector, 0.003, 0.001, 0.0007, nodes$label, 0.003,
           "Pagerank Centrality")

```

```{r graph_eigen, echo=FALSE, warning=FALSE}
plot_graph(network_graph, "dh", eigenv$vector, 0.55, 0.04, 0.00005, nodes$label, 0.55, 
           "Eigenvector Centrality")
```

Top 5 nodes for Eigenvector and Pagerank are also presented:

```{r echo=FALSE}
print("top 10 nodes for Pagerank")
V(network_graph)$label[order(prank$vector, decreasing=TRUE)][1:10]
print("top 10 nodes for Eigenvector")
V(network_graph)$label[order(eigenv$vector, decreasing=TRUE)][1:10] 
```

From the results, it's possible to see that a lot of the diseases that have high values for this measures are related to various form of cancer. Colon Cancer takes the firts place for both Pagerank and Eigenvector.

# Clustering

## Clustering Coefficient
Clustering Coefficient is used to measure how dense the network is. High values of Clustering Coefficient means that the network presents a high number of edges, and that is likely to have a lot of clusters in the network

```{r echo=FALSE}
g_trans <- transitivity(network_graph, type="global",isolates="zero")
l_trans <- transitivity(network_graph, type="local",isolates="zero")
transitivity_recap <- data.frame(matrix(ncol=1, nrow=2))
colnames(transitivity_recap) <- "Values"
rownames(transitivity_recap) <- c("Local Transitivity Average","Global Transitivity")
transitivity_recap[1,1] <- mean(l_trans)
transitivity_recap[2,1] <- g_trans
pander(transitivity_recap,digits=4,justify="center")
```

Values for the network are quite low for both local and global transitivity values, and so evaluating community detection algorithms on this network could be quite difficult. Giving the fact that, for every measure of centrality analyzed, nodes representing genes had always the lowest scores, it's better to remove them from the network, cause probably they are just contributing to create confusion during the clustering phase. Moreover, in this particular network two nodes are connected only if the diseases represented are sharing a common gene, so the presence of the genes itself became redundant.
On top of that, edges weight has been tweaked to represent the number of genes shared by the nodes connected by a certain edge and the network has been transformed to an undirected one to facilitate operations.


```{r echo=FALSE}
#Moving To a Non-Directed Graph to facilitate the Clustering Analysis, Recalculating Weights
for(i in 1:length(edges[, 5])){
  e = edges[i, ]
  source_node = V(network_graph)[as.character(e$Source)]
  target_node = V(network_graph)[as.character(e$Target)]
  neigh_source_node = neighborhood(network_graph,nodes=source_node)[[1]]
  neigh_target_node = neighborhood(network_graph,nodes=target_node)[[1]]
  neigh_source_node = as.numeric(neigh_source_node[neigh_source_node&X1 == "gene"])
  neigh_target_node = as.numeric(neigh_target_node[neigh_target_node&X1 == "gene"])
  new_weight = length(intersect(neigh_source_node,neigh_target_node))
  edges[i, ]$weight = new_weight
}

new_graph = graph.data.frame(edges,directed=FALSE,vertices=nodes)

#REMOVING GENES
network_graph_no_genes = induced_subgraph(new_graph,which(nodes$X1 != "gene"))
network_graph_no_genes = igraph::simplify(network_graph_no_genes)
ggraph(network_graph_no_genes, layout="graphopt") +
  geom_edge_fan(aes(width=E(network_graph_no_genes)$weight), colour= "gray66",show.legend = FALSE) +
  geom_node_point(fill= "#00FF00",
                  shape=21, col="gray15", show.legend = FALSE) +
  scale_edge_width_continuous(range=c(0.2,0.9)) +
  scale_size_continuous(range=c(1, 10)) +
  theme_graph(base_size = 11, base_family = "sans") +
  ggtitle("Human Disease Network With Genes Removed") 

paste("Nodes: ", vcount(network_graph_no_genes))
paste("Edges: ", ecount(network_graph_no_genes))

```


The network in this state is well manageable and easily visualizable, with less nodes and less edges. It's possible to see that some of the clusters are becoming more visibile.
It's also in a better state regarding Clustering Coefficient measures, as the new results are way better than the previous one.

```{r echo=FALSE}
g_trans <- transitivity(network_graph_no_genes, type="global",isolates="zero")
l_trans <- transitivity(network_graph_no_genes, type="local",isolates="zero")
transitivity_recap <- data.frame(matrix(ncol=1, nrow=2))
colnames(transitivity_recap) <- "Values"
rownames(transitivity_recap) <- c("Local Transitivity Average","Global Transitivity")
transitivity_recap[1,1] <- mean(l_trans)
transitivity_recap[2,1] <- g_trans
pander(transitivity_recap,digits=4,justify="center")
```


## Algorithms
Without any attribute related to nodes, it's impossbile to go for traditional clustering algorithms, and so community detection and graph partitioning ones are the way to go. 
The chosen algorithms for this project are:
- Girvann-Newmann
- Louvain
- Leiden
- Fastgreedy
- Markov
- Leading Eigenvector
Those algortithms have been chosen after looking at some state-of-the-art works on clustering techniques.
Prior to show result of the analisys, it's worth noting that every cluster will be labeled via the most frequent label inside of it, and that the final results will be stored in a matrix that will be used for a Groundtruth comparison.

```{r echo=FALSE}
#Function to label clusters
label_cluster <- function(clusters)
{
  cluster_df <-
    as.data.frame(matrix(
      1:length(clusters),
      nrow = length(clusters),
      dimnames = list(NULL, "id")
    ))
  
  for (c in 1:length(clusters))
  {
    labels <- V(network_graph_no_genes)[clusters[[c]]]$X1
    labels <- labels[which(labels != "Multiple" & labels != "Unclassified")]
    
    if (length(labels) != 0)
    {
      value <- which.max(unlist(table(labels)))
      cluster_df$name[cluster_df$id == c] <- names(value)[1]
    } else
    {
      cluster_df$name[cluster_df$id == c] <- V(network_graph_no_genes)[clusters[[c]]]$X1[1]
    }
  }
  
  return(cluster_df)
}
#defining data-frame to compare clusters
clustering_result <- nodes %>% filter(X1 != "gene") %>% select(-X0)

```


## Girvan-Newmann

Girvan-Newmann algorithm works with the idea that edges that are connecting different clusters of the network should have a high value of edge betweenness. It's considered an Hierarch-centric algorithm

```{r echo=FALSE, message=FALSE, warning=FALSE}
  girvan_newmann = cluster_edge_betweenness(network_graph_no_genes,directed=FALSE,
                                            weights=E(network_graph_no_genes)$weight) 
  girvannewmann_df = label_cluster(girvan_newmann)
  clustering_result$girvannewmann <- NA
  for (n in 1:length(girvan_newmann$membership))
  {
    clustering_result$girvannewmann[n] = girvannewmann_df$name[girvan_newmann$membership[n]]
  }
  plot_clustering_graph_with_legend(network_graph_no_genes, "graphopt", clustering_result$girvannewmann,
                           E(network_graph_no_genes)$weight, "Girvan Newman")
  
  print(paste("GN Number Of Communities:",max(girvan_newmann$membership)))

```


## Louvain and Leiden 
Those two algorithms are presented toghether cause Leiden is considered a natural evolution of Louvain. Louvain is a clustering algorithm based on the concept of modularity maximization. Leiden is a more recent and updated version of this algorithm, way faster and with the possibility of decide pre execution the number of clusters. It's expected to be the most performant algorithm from those selected for this project for those reasons.

```{r echo=FALSE,message=FALSE,warning=FALSE}
louvain_clustering = cluster_louvain(network_graph_no_genes, weights = E(network_graph_no_genes)$weights)

print(paste("Louvain Number Of Communities:",max(louvain_clustering$membership)))
louvain_df = label_cluster(louvain_clustering)
clustering_result$Louvain <- NA
for (n in 1:length(louvain_clustering$membership))
{
  clustering_result$Louvain[n] = louvain_df$name[louvain_clustering$membership[n]]
}
plot_clustering_graph_with_legend(network_graph_no_genes, "graphopt", clustering_result$Louvain,
                                  E(network_graph_no_genes)$weight, "Louvain")
##Clustering Section
libraries = c("scales","gridExtra","leiden","pander","MCL","caret","aricode")
import_libraries(libraries)
mat = as_adj(network_graph_no_genes, type = "both", attr = "weight")
leiden_clustering <- leiden(mat, resolution_parameter = 86)
print(max(leiden_clustering))
leiden_clustering <- make_clusters(network_graph_no_genes, membership = leiden_clustering)
leiden_df <- label_cluster(leiden_clustering)
clustering_result$Leiden = NA
for (n in 1:length(leiden_clustering$membership))
{
  clustering_result$Leiden[n] <- leiden_df$name[leiden_clustering$membership[n]]
}
plot_clustering_graph_with_legend(network_graph_no_genes, "graphopt", clustering_result$Leiden,
                         E(network_graph_no_genes)$weight, "Leiden")
```


from the results, we can see that the improvement is tangible, with Leiden detecting a lot more cluster than Louvain. This will surely impact on measures like purity

## Markov

```{r echo=FALSE, message=FALSE, warning=FALSE}
##Markov Clustering
mat = as_adj(network_graph_no_genes,type = "both",attr = "weight")
markov_clustering = mcl(mat,addLoops = FALSE, inflation=4, allow1 = TRUE)
print(paste("Markov Communities:",markov_clustering$K))
markov_df <- as.data.frame(matrix(1:(markov_clustering$K), nrow = markov_clustering$K, 
                                          dimnames = list(NULL, "id")))
markov_df$name <- NA
markov_df$id <- unique(markov_clustering$Cluster)
for (id in markov_df$id)
{
  labels <- V(network_graph_no_genes)[markov_clustering$Cluster == id]$X1
  labels <- labels[labels != "Multiple"]
  
  value <- which.max(unlist(table(labels)))
  
  if (length(value) != 0)
  {
    markov_df$name[markov_df$id == id] <- names(value)[1]
  }
  else
  {
    markov_df$name[markov_df$id == id] <- "Multiple"
  }
}
clustering_result$Markov <- NA
for (n in 1:length(markov_clustering$Cluster))
{
  clustering_result$Markov[n] <- markov_df$name[markov_clustering$Cluster[n] == markov_df$id]
}
plot_clustering_graph_with_legend(network_graph_no_genes, "graphopt", clustering_result$Markov,
                         E(network_graph_no_genes)$weight, "Markov Clusters")
```


also in this case, it's possible to see that a the number of clusters identifed is way higher than Louvain and Girvann-Newmann: Markov algorithm works on the principle that a random walk in a graph G that visits a cluster will exit from it only when it has visited the majority of his edges.

## Fastgreedy

Fastgreedy is another algorithm that works on Modularity Maximization. It starts by taking every node as a community, and iterating over the network it groups nodes to get to the limit of the modularity.

```{r echo=FALSE}
fastgreedy_clusters  = cluster_fast_greedy(network_graph_no_genes, modularity = TRUE,
                                           weights = E(network_graph_no_genes)$weight)

print(paste("Fastgreedy communities: ", max(fastgreedy_clusters$membership)))
fgreedy_df <- label_cluster(fastgreedy_clusters)
clustering_result$Fastgreedy <- NA
for (n in 1:length(fastgreedy_clusters$membership))
{
  clustering_result$Fastgreedy[n] <- fgreedy_df$name[fastgreedy_clusters$membership[n]]
}
plot_clustering_graph_with_legend(network_graph_no_genes, "graphopt", clustering_result$Fastgreedy,
                         E(network_graph_no_genes)$weight, "Fastgreedy")
```


results, as for the other algorithms related to modularity maximization, are pretty low, with just 26 clusters found.

## Leading Eigenvector

Leading Eigenvector is another modularity maximization based algorithm that works on the principle of dividing the network in two components at every iteration, reaching the maximum limit of modularity. As for the other modularity maximization algorithms, it's expected to perform badly because of the difficulty of recognizing clusters with just 1 element

```{r echo=FALSE, message=FALSE, warning=FALSE}
lead_eigen <- cluster_leading_eigen(network_graph_no_genes, weights = E(network_graph_no_genes)$weight)
print(paste("Leading Eigen Communities: ", max(lead_eigen$membership)))
lead_eigen_df <- label_cluster(lead_eigen)
clustering_result$Lead_eigen = NA
for (n in 1:length(lead_eigen$membership))
{
  clustering_result$Lead_eigen[n] <- lead_eigen_df$name[lead_eigen$membership[n]]
}
plot_clustering_graph_with_legend(network_graph_no_genes, "graphopt", clustering_result$Lead_eigen,
                         E(network_graph_no_genes)$weight, "Leading eigenvector Clusters")

```


## Groundtruth

as said before, the network presents 22 different classes of diseases that nodes are associated with. Plotting a graph highlighting those classes can be useful to evidence if nodes belonging to the same class are close to each other.

```{r echo=FALSE,message=FALSE,warning=FALSE}
groundtruth_clusters = unique(V(network_graph_no_genes)$X1)
i = 1
for (value in groundtruth_clusters){
  V(network_graph_no_genes)[V(network_graph_no_genes)$X1 == value]$color = i
  i = i + 1
}

plot_clustering_graph_with_legend(network_graph_no_genes, "graphopt", nodes$X1[nodes$X0 =="disease"], E(network_graph_no_genes)$weight, "Groundtruth Clustering")

```

as expected, nodes associated with the cancer class are very close, with few outliers, while for example nodes associated with Nutritional and Neurological classes are more sparse in the network

# Final Comparison

## Purity

Purity is a measure that indicates how well a community can represent an entire class. In other words, purity indicates how many nodes for a class have been classified correctly in a cluster. Value ranges from 0 to 1, with one only reached if a cluster is completely representing an entire class.
Looking and the numbers from the various algorithms used, it's expected that Leiden will have the higher value of purity, because it's detecting even the smallest clusters in the network.


```{r echo=FALSE, message=FALSE, warning=FALSE}
#Purity
p_frame = data.frame(matrix(ncol = 23, nrow = 7))
colnames(p_frame) = c(groundtruth_clusters,"Purity Measure")
rownames(p_frame) = c("newmann","louvain","leiden","markov","fastgreedy","lead_eigen","avg")

for(i in 4:length(clustering_result)){
  cmatrix = confusion_matrix(clustering_result$X1,clustering_result[,i])
  p_frame[i-3, ]=unname(c(diag(cmatrix$table/rowSums(cmatrix$table)),
                          cmatrix$overall[1])) 
}
p_frame$Multiple = NULL
p_frame$Unclassified = NULL
p_frame[7, ] = colMeans(p_frame[c(1:6), ])
p_frame = as.data.frame(t(p_frame))
panderOptions('table.split.table',8*15)
pander(format(p_frame))


```

as expected, Leiden is scoring 0.81, while especially modularity maximization based algortihms are scoring a maximum of 0.6 in purity measure. Markov is the second top scorer, with a value of 0.74, close to the Leiden one.

## Normalized Mutal Information

Normalized Mutual Information is the normalized version of Mutual Information. Mutual Information measures the mutual dependence between two clusters in the network.

```{r echo=FALSE, message=FALSE, warning=FALSE}
mutualinfo_df <- data.frame(matrix(ncol = 1, nrow = 6))
colnames(mutualinfo_df) <- c("NMI")
rownames(mutualinfo_df) <- c("newmann", "louvain", "leiden", "markov",
                      "fastgreedy", "lead_eigen")

for (i in 1:nrow(mutualinfo_df))
{
  groundtruth = clustering_result[, 3]
  clusters = clustering_result[, 3 + i]
  groundtruth[groundtruth == "Multiple"] <- clusters[groundtruth == "Multiple"]
  groundtruth[groundtruth == "Unclassified"] <- clusters[groundtruth == "Unclassified"]
  mutualinfo_df[i, ] <- NMI(groundtruth, clusters)
}
pander(format(mutualinfo_df))

```

Leiden has a pretty high value for Mutual Information compared to the other Modularity Maximization algorithms, scoring above 0.7, Markov came second for this measure too, with a value of 0.64, close to the Leiden one.

## Adjusted Rand Index

The Adjusted Rand Index is the corrected-for-chance version of the Rand index. Such a correction for chance establishes a baseline by using the expected similarity of all pair-wise comparisons between clusterings specified by a random model.

```{r echo=FALSE,warning=FALSE,message=FALSE}
rand_df = data.frame(matrix(ncol=1,nrow=6))
colnames(rand_df) = c("Adjusted Rand Index")
rownames(rand_df) = c("newmann", "louvain", "leiden", "markov",
                      "fastgreedy", "lead_eigen")
for (i in 1:nrow(rand_df))
{
  groundtruth <- clustering_result[, 3]
  clusters <- clustering_result[, 3 + i]
  groundtruth[groundtruth == "Multiple"] <- clusters[groundtruth == "Multiple"]
  groundtruth[groundtruth == "Unclassified"] <- clusters[groundtruth == "Unclassified"]
  rand_df[i, ] <- ARI(groundtruth, clusters)
}
pander(format(rand_df))


```

even for this measure, Leiden and Markov are still the top performers, scoring values above 0.5 and getting closer to 1.

# Concusions and Further Developments

The analysis presented proved that finding clusters on this network was a pretty hard task: the majority of the algorithms proposed in this paper scored badly for clustering measures and just Leiden algorithm and Markov Algorithm came close to a good result in finding clusters. The analisys was mainly conducted focusing on old state-of-the-art Modularity Maximization based algorithms, that proved to not be a good choice overall. if a choice has to be made, Leiden is the right algorithm for a task like this one, considering the fact that is a pretty new discovery (2019) and it's an improved version of Louvain algorithm, another famous state-of-the-art algorithm. 
Further developments on this task include trying other categories of algorithms, like the graph partitioning ones, or maybe try a complete analisys over the original network, without removing genes, to prove the fact that they are just adding more confusion in the clustering process
